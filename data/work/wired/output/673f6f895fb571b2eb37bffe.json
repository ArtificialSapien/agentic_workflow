{
  "title": "Researchers hacked several robots infused with large language models, getting them to behave dangerouslyâ€”and pointing to a bigger problem ahead.",
  "date": "2024-12-07",
  "content": "Researchers were able to persuade a simulated self-driving car to ignore stop signs and even drive off a bridge. They also got a wheeled robot to find the best place to detonate a bomb, and force a four-legged robot to spy on people and enter restricted areas. LLM-powered robots can easily be hacked so that they behave in potentially dangerous ways.",
  "author": "Will Knight",
  "source": "https://www.wired.com/story/researchers-llm-ai-robot-violence/"
}
